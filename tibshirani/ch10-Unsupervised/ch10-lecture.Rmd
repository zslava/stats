---
title: "ch10 Lecture"
author: "S Zimine"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Principal Components
===========================

Working with `USArrests` data from base R.
```{r}
data("USArrests")
dim(USArrests)
str(USArrests)
head(USArrests)
summary(USArrests)
```

Lets stardadize features. It fineds 4 principal components.

```{r}
pca.out <- prcomp(USArrests, scale=TRUE) #which scaling
pca.out
names(pca.out)
```

Rotation are loadings (projects) of each principal component to original features.

E.g  PC1 is an averaging of all 3 types of crimes and PC2 is more expressed as a func of UrbanPop. 

```{r}
biplot(pca.out,scale=0, cex=0.6) ##useful visualization
```



K-Means Clustering
==================

K-means works in any dimensions. For the demo we will proceed with 2D example


```{r}
set.seed(101)
n <- 100
x<-matrix(rnorm(n*2),n,2)
xmean <- matrix( rnorm(8,sd=4),4,2) #4 rows 2 columns. rows are stonly pushed apart
which <- sample(1:4, n, replace=TRUE)
x <- x+xmean[which,] # by adding push apart means we generated clustered data
plot(x,col=which, pch=19)
```


We kno the "true" cluste IDs, but we wont tell it to `kmeans` algo

```{r}
km.out <- kmeans(x,4,nstart=15) #15 random starts
km.out
```

(between_SS / total_SS =  87.6 %)  suggest tht k-means did a good job separating data in clean clusters 
```{r}
plot(x,col=km.out$cluster, cex=2,pch=1,lwd=2)
points(x,col=c(4,3,2,1)[which], pch=19  )
```

There are 2 mismatches on 100 points.



Hierarchical Clustering
========================


```{r}
hc.complete <- hclust(dist(x), method="complete")
plot(hc.complete)

#singlie linking
hc.single <- hclust(dist(x), method="single")
plot(hc.single)

#avereage linking
hc.avg <- hclust(dist(x), method="average")
plot(hc.avg)

```


The Full Dendrogram suggest we have 4 most significant culsters Method `complete` appears to be better working.


Lets compare this with the actual clusters in the data We will use the function `cutree` to cut the tree at level 4

```{r}
hc.cut <- cutree(hc.complete,4)
cm<- table(predict=hc.cut,truth=which)
cm
##error rate
1 - sum(diag(cm))/sum(cm)

cm <- table(predict=hc.cut, k_m=km.out$cluster)
print(cm)
```

or we can use our group membership as labels for the leaves of the dendrogram:
```{r}
plot(hc.complete, labels=which)
```

